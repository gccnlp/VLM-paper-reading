# 技术报告
## 2025.7-2025.12
### Kwai Keye-VL 1.5 Technical Report
#### 1. 架构
  * Qwen3-8B和SigLIP-400M-384-14做热启
  * 快慢帧技术，如果本帧和上一帧变化不超过95%就是快帧，否则为慢帧（第一帧是永远是慢帧），本质上就是慢帧高分，快帧低分来在总token预算下实现训练。
<img width="1489" height="405" alt="image" src="https://github.com/user-attachments/assets/28bc3c7b-e7c6-43ad-8142-6ba104e273f6" />

#### 2. 配置
  * 视觉token数量为20K（没有时序融合，同时patch size为14，视频总最大像素数为16056320）
  * 快慢帧会有特殊token进行标记
#### 3. 数据
  * 视频来自多样化的开源数据集（ShareGPT4V、Pandas等）和大规模高质量的内部视频数据。
  * 打标方案（0.5-1-2的fps来进行打标）
  * Frame-level OCR任务：（1）打乱视频帧，模型需要预测顺序（不过这个可能对带有时间戳的模型不重要）。（2）给一组相关的video，识别出相关的。
#### 4. 预训练-总共4阶段
  * 第一阶段-ViT预训练（只训练ViT），第二阶段跨模态预训练（只训练MLP Projector，8K长度）
  * 第三阶段-多任务预训练（我们的scope）
    * 8K长度，数据包含常见视觉语言任务，包括图像字幕、光学字符识别 (OCR)、接地、视觉问答 (VQA) 和交错的图像文本数据。
  * 第四阶段-退火（我们的scope）
    * 长度从8K扩展到128K，context并行训练。数据比例24% videos+50% images+26%纯文
#### 5. 后训练-总共5阶段
  * Non-Reasoning阶段
    * SFT
      * 总共7.5M的数据，对每个sample roll出来多个轨迹，如果正确率太高就筛掉。
      * 进一步退火，训练末期以较低的学习率退火（类似Qwen的做法）
    * MPO通过配对优化偏好
  * Reasoning阶段
    * SFT冷启动
      * 面向困难任务，如果任务简单就多个简单任务合并为复杂任务，同时使用置信度筛选推理轨迹，以及非常复杂的数据筛选准则（此处不展开）。
      * 针对弱点任务（OCR），构建专家模型和通用模型做model merge
    * 迭代式RL
      * 对于困难样本，通过离线的拒绝采样训进去，然后和在线的GRPO交替进行。
      * 这个具体思路比较有意思：主要是先把冷启动模型做RL；然后在冷启动数据上拒绝采样更新数据集，训冷启动模型；把新的冷启动模型做RL...
    * 对齐RL
      * 多样化奖励设计
### GLM-4.5V / GLM-4.1V-Thinking / GLM-4.6V Technical Report
#### 1. 架构
* 类似Qwen3-VL，使用AIMv2-Huge+GLM-4
<img width="787" height="431" alt="image" src="https://github.com/user-attachments/assets/182faf37-e930-4776-af8d-0c76465dea6c" />

#### 2. 数据
* 广泛任务类别
#### 3. 训练流程
* 多模态预训练：seq=8192，global batch=1536，训练 120K steps；使用数据 packing 提升效率
* 长文扩展：（1）加入视频 + 超 8K 的长序列交错图文；把 seq 拉到 32768，并引入 context parallel=4；额外训练 10K steps（batch 仍 1536）。（ GLM-4.6V 上下文扩到 131072，额外 2K steps（global batch=128））。
* SFT：nothinking和thinking同时包含的数据，长度为32K
* RL：人工筛选样本做课程学习，根据不同的任务给不同的奖励设计
### InternVL3.5 Technical Report
#### 1. 架构
* Vision Encoder：使用 InternViT-300M 和 InternViT-6B。
* LLM：基于 Qwen3 系列 (0.6B-72B) 和 GPT-OSS (MoE) 进行初始化，覆盖 1B 到 241B 参数规模。
* 动态分辨率：保留了 InternVL 1.5 的动态高分辨率（Dynamic High Resolution）策略。
引入 Visual Resolution Router (ViR)：标准模式下图像 Patch 转为 256 tokens；Flash 模式下，Router 根据语义丰富程度判断，若信息密度低则通过 Pixel Shuffle 压缩至 64 tokens。
<img width="1290" height="429" alt="image" src="https://github.com/user-attachments/assets/f8cc293a-565f-4774-9a94-732a441cda30" />

#### 2. 数据
* 预训练数据：
  * 规模：~116M 样本，约 250B tokens。
  * 配比：纯文本 : 多模态  1 : 2.5。
  * 来源：多模态数据来自 InternVL3 语料库；纯文本来自 InternLM 系列及开源数据集。
* SFT数据：
  * 规模：~56M 样本，约 130B tokens。
  * 配比：纯文本 : 多模态  1 : 3.5。
* RL 数据：
  * MMPR-v1.2 (200K 样本对) 用于 Offline RL。
  * MMPR-Tiny (70K Query) 用于 Online RL，筛选准确率在 0.2-0.8 之间的样本。
#### 3. 训练流程
* Stage 0: 预训练（未知具体，32K上下文）
* Stage 1: Supervised Fine-Tuning (SFT)
  * 目标函数同预训练（NTP），使用 Square Averaging 平衡长短回复的 Loss 权重。
  * Context Window 设为 32K，引入 Random JPEG Compression 增强鲁棒性。
* Stage 2: Cascade Reinforcement Learning (Cascade RL)
  * Offline RL (MPO)
    * Loss 组合：Preference Loss (DPO) + Quality Loss (BCO) + Generation Loss (LM)。
    * 作用：作为 Warm-up，低成本快速提升模型性能，防止 Reward Hacking。
  * Online RL (GSPO) 
* Stage 3: Visual Consistency Learning (ViCO) [仅针对 Flash 模型]
  * Consistency Training：最小化“高压缩率视觉特征生成的回复”与“低压缩率视觉特征生成的回复”之间的 KL 散度。
  * Router Training：冻结主模型，仅训练 Router。计算压缩前后 Loss 的比率 ，若  超过阈值则标记为不可压缩，以此监督 Router 分类。
### Kimi K2.5 Technical Report
> 开源多模态 Agentic 模型，强调文本与视觉的联合优化，并引入 Agent Swarm 并行智能体框架。总参数 1.04T（MoE），激活参数 32B。

#### 1. 架构
* **Vision Encoder — MoonViT-3D**：基于 SigLIP-SO-400M 初始化，采用 NaViT 的 patch packing 策略，支持原生分辨率输入（无需切图拼接）。
  * 图像：将不同分辨率图像的 patch 展平后拼接为 1D 序列，实现变分辨率高效训练。
  * 视频：将连续 4 帧视为一个时空体积（spatiotemporal volume），2D patch 在空间+时间维度上联合展平并 pack 到同一序列中，图像和视频编码器**完全共享权重**。在 MLP Projector 之前做轻量级 temporal pooling（4 帧取平均），实现 **4× 时序压缩**，可处理 4 倍长的视频。
* **MLP Projector**：连接 MoonViT-3D 和 LLM。
* **LLM — Kimi K2 MoE**：总参数 1.04T，激活 32B，384 个专家取 8 个（稀疏度 48），使用 MuonClip 优化器 + QK-Clip 保证训练稳定性。

#### 2. 核心Insight
##### 2.1 文本-视觉联合预训练（Early Fusion）
* **关键发现**：在固定总 token 预算下，**早期融合 + 低视觉比例**（10% vision : 90% text）优于晚期融合 + 高视觉比例（50%:50%）。
* 具体做法：从训练一开始就以恒定比例混入视觉 token，而非传统方法在后期才加入大量视觉数据。

| 视觉注入时机 | Vision:Text 比例 | 视觉知识 | 视觉推理 | OCR | 文本知识 | 文本推理 | 代码 |
|---|---|---|---|---|---|---|---|
| Early (0%) | 10%:90% | **25.8** | **43.8** | **65.7** | **45.5** | 58.5 | **24.8** |
| Mid (50%) | 20%:80% | 25.0 | 40.7 | 64.1 | 43.9 | **58.6** | 24.0 |
| Late (80%) | 50%:50% | 24.2 | 39.0 | 61.5 | 43.1 | 57.8 | 24.0 |

##### 2.2 Zero-Vision SFT
* **核心思路**：后训练阶段**仅用纯文本 SFT 数据**即可激活视觉推理和工具调用能力，不需要人工标注的视觉 CoT 轨迹。
* 所有图像操作通过 IPython 中的程序化操作来代理（如二值化计数估算物体大小、目标定位、OCR等）。
* 实验发现：加入人工设计的 text-vision SFT 轨迹反而**损害泛化**，纯文本 SFT 效果更好——因为联合预训练已经建立了强 vision-text 对齐，能力可以自然跨模态迁移。

##### 2.3 视觉 RL 提升文本性能（跨模态迁移）
* 视觉 RL 训练后，纯文本 benchmark 也提升：MMLU-Pro (84.7→86.4)、GPQA-Diamond (84.3→86.4)、LongBench v2 (56.7→58.9)。
* 这说明视觉 RL 增强了结构化信息提取能力，对类似视觉推理的文本任务（如计数、OCR类）也有帮助。

##### 2.4 联合多模态 RL
* 不按模态划分专家，而是**按能力划分**（知识、推理、编码、Agentic 等），每个领域专家同时从纯文本和多模态查询中学习。
* GRM（Generative Reward Model）也跨模态优化，不设模态壁垒。

#### 4. 训练流程
##### 4.1 预训练（3 个阶段，共 ~16T tokens）

| 阶段 | 数据 | 序列长度 | Tokens | 可训练模块 |
|---|---|---|---|---|
| ViT Training | Alt text、合成 Caption | 4096 | 1T | 仅 ViT |
| Joint Pre-training | Grounding、OCR、视频 + 文本、知识、交错数据 | 4096 | 15T | ViT & LLM |
| Long-context Mid-training | 视频、OS 截图 + 高质量文本&多模态、长文本、长视频、推理、Long-CoT | 32768→262144 | 500B→200B | ViT & LLM |

* Joint Pre-training 从 Kimi K2 接近末尾的 checkpoint 继续训练，增加编码相关数据权重。
* Long-context 阶段通过 YaRN 插值逐步扩展上下文长度，显著提升长文本理解和长视频理解。

##### 4.2 后训练
* **SFT**：从 K2、K2 Thinking 及内部专家模型合成高质量候选回复，结合人工标注 + prompt 工程 + 多阶段验证，生成大规模指令调优数据集。
* **RL**：
  * 使用统一 Agentic RL 环境，支持文本-视觉联合 RL 和 PARL。
  * **Policy Optimization**：引入 token 级 clipping 机制，基于 log-ratio 显式约束 off-policy 发散。
  * **GRM**（Generative Reward Model）：不仅评判对错，还细粒度评估 helpfulness、上下文相关性、细节层次、美学质量、指令遵循等。使用多套 GRM rubric 避免 reward hacking。
  * **Toggle**（Token Efficient RL）：交替在"预算受限"和"标准缩放"两个阶段之间切换，解决长度过拟合问题——既保持 token 效率又不牺牲推理能力。
