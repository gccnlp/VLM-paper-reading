# 技术报告
## 2025.6-2025.12
### Kwai Keye-VL 1.5 Technical Report
#### 1. 架构
  * Qwen3-8B和SigLIP-400M-384-14做热启
  * 快慢帧技术，如果本帧和上一帧变化不超过95%就是快帧，否则为慢帧（第一帧是永远是慢帧），本质上就是慢帧高分，快帧低分来在总token预算下实现训练。
<img width="1489" height="405" alt="image" src="https://github.com/user-attachments/assets/28bc3c7b-e7c6-43ad-8142-6ba104e273f6" />

#### 2. 配置
  * 视觉token数量为20K（没有时序融合，同时patch size为14，视频总最大像素数为16056320）
  * 快慢帧会有特殊token进行标记
#### 3. 数据
  * 视频来自多样化的开源数据集（ShareGPT4V、Pandas等）和大规模高质量的内部视频数据。
  * 打标方案（0.5-1-2的fps来进行打标）
  * Frame-level OCR任务：（1）打乱视频帧，模型需要预测顺序（不过这个可能对带有时间戳的模型不重要）。（2）给一组相关的video，识别出相关的。
#### 4. 预训练-总共4阶段
  * 第一阶段-ViT预训练（只训练ViT），第二阶段跨模态预训练（只训练MLP Projector，8K长度）
  * 第三阶段-多任务预训练（我们的scope）
    * 8K长度，数据包含常见视觉语言任务，包括图像字幕、光学字符识别 (OCR)、接地、视觉问答 (VQA) 和交错的图像文本数据。
  * 第四阶段-退火（我们的scope）
    * 长度从8K扩展到128K，context并行训练。数据比例24% videos+50% images+26%纯文
#### 5. 后训练-总共5阶段
  * Non-Reasoning阶段
    * SFT
      * 总共7.5M的数据，对每个sample roll出来多个轨迹，如果正确率太高就筛掉。
      * 进一步退火，训练末期以较低的学习率退火（类似Qwen的做法）
    * MPO通过配对优化偏好
  * Reasoning阶段
    * SFT冷启动
      * 面向困难任务，如果任务简单就多个简单任务合并为复杂任务，同时使用置信度筛选推理轨迹，以及非常复杂的数据筛选准则（此处不展开）。
      * 针对弱点任务（OCR），构建专家模型和通用模型做model merge
    * 迭代式RL
      * 对于困难样本，通过离线的拒绝采样训进去，然后和在线的GRPO交替进行。
      * 这个具体思路比较有意思：主要是先把冷启动模型做RL；然后在冷启动数据上拒绝采样更新数据集，训冷启动模型；把新的冷启动模型做RL...
    * 对齐RL
      * 多样化奖励设计
### InternVL3.5 Technical Report
#### 1. 架构
* Vision Encoder：使用 InternViT-300M 和 InternViT-6B。
* LLM：基于 Qwen3 系列 (0.6B-72B) 和 GPT-OSS (MoE) 进行初始化，覆盖 1B 到 241B 参数规模。
* 动态分辨率：保留了 InternVL 1.5 的动态高分辨率（Dynamic High Resolution）策略。
引入 Visual Resolution Router (ViR)：标准模式下图像 Patch 转为 256 tokens；Flash 模式下，Router 根据语义丰富程度判断，若信息密度低则通过 Pixel Shuffle 压缩至 64 tokens。
<img width="1290" height="429" alt="image" src="https://github.com/user-attachments/assets/f8cc293a-565f-4774-9a94-732a441cda30" />

#### 2. 数据
* 预训练数据：
  * 规模：~116M 样本，约 250B tokens。
  * 配比：纯文本 : 多模态  1 : 2.5。
  * 来源：多模态数据来自 InternVL3 语料库；纯文本来自 InternLM 系列及开源数据集。
* SFT数据：
  * 规模：~56M 样本，约 130B tokens。
  * 配比：纯文本 : 多模态  1 : 3.5。
* RL 数据：
  * MMPR-v1.2 (200K 样本对) 用于 Offline RL。
  * MMPR-Tiny (70K Query) 用于 Online RL，筛选准确率在 0.2-0.8 之间的样本。
#### 3. 训练流程
* Stage 0: 预训练（未知具体，32K上下文）
* Stage 1: Supervised Fine-Tuning (SFT)
  * 目标函数同预训练（NTP），使用 Square Averaging 平衡长短回复的 Loss 权重。
  * Context Window 设为 32K，引入 Random JPEG Compression 增强鲁棒性。
* Stage 2: Cascade Reinforcement Learning (Cascade RL)
  * Offline RL (MPO)
    * Loss 组合：Preference Loss (DPO) + Quality Loss (BCO) + Generation Loss (LM)。
    * 作用：作为 Warm-up，低成本快速提升模型性能，防止 Reward Hacking。
  * Online RL (GSPO) 
* Stage 3: Visual Consistency Learning (ViCO) [仅针对 Flash 模型]
  * Consistency Training：最小化“高压缩率视觉特征生成的回复”与“低压缩率视觉特征生成的回复”之间的 KL 散度。
  * Router Training：冻结主模型，仅训练 Router。计算压缩前后 Loss 的比率 ，若  超过阈值则标记为不可压缩，以此监督 Router 分类。
